<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RoboMIND2.0</title>
  <link rel="icon" href="./figs/logo.png" type="image/png">
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="toc">
    <h3>Content</h3>
    <hr>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#collection-platform">Collection Platform</a></li>
      <li><a href="#realworld-setup">Real-world Setup</a></li>
      <li><a href="#realworld-results">Real-World Results</a></li>
      <li><a href="#dual-system-experiments">Dual System Experiments</a></li>
      <!-- <li class="toc-subsection"><a href="#training-in-sim">Training in Sim</a></li> -->
    </ul>
  </div>

  <div class="main-content">
    <div class="hero-text" style="font-size: 100px; text-align: center; margin-bottom: -10px;">RoboMIND2.0</div>
    <div class="sub-hero-text" style="text-align: center;">A Multimodal, Bimanual Mobile
Manipulation Dataset for Generalizable Embodied Intelligence</div>

    <!-- add authors -->
    <div class="authors" style="text-align: center; color: #000000;">
      <div class="author-block" style="font-weight: bold; line-height: 1.6;">
        Chengkai Hou<sup>1,2,*,&#8225;</sup>, Kun Wu<sup>1,*,&#8225;</sup>, Jiaming Liu<sup>2,*,&#8225;</sup>, Zhengping Che<sup>1,*,&#8224;</sup>, Di Wu<sup>1,2,*</sup>,<br>
        Fei Liao<sup>1,2,*</sup>, Guangrun Li<sup>1,2,*</sup>, Jingyang He<sup>1,2,*</sup>, Qiuxuan Feng<sup>1,2,*</sup>, Zhao Jin<sup>1,*</sup>,<br>
        Chenyang Gu<sup>2</sup>, Zhuoyang Liu<sup>2</sup>, Nuowei Han<sup>2</sup>, Xiangju Mi<sup>2</sup>, Yaoxu Lv<sup>2</sup>,<br>
        Yankai Fu<sup>2</sup>, Gaole Dai<sup>2</sup>, Langzhe Gu<sup>2</sup>, Tao Li<sup>1</sup>, Yuheng Zhang<sup>1</sup>, Xinhua Wang<sup>1</sup>,<br>
        Shichao Fan<sup>1</sup>, Yixue Zhang<sup>1</sup>, Meng Li<sup>1</sup>, Zhen Zhao<sup>1</sup>, Ning Liu<sup>1</sup>,<br>
        Zhiyuan Xu<sup>1</sup>, Pei Ren<sup>1</sup>, Junjie Ji<sup>1</sup>, Haonan Liu<sup>1</sup>,<br>
        Kuan Cheng<sup>2</sup>, Shanghang Zhang<sup>2,&#9993;</sup>, Jian Tang<sup>1,&#9993;</sup>
      </div>

      <div class="affiliation" style="font-weight: normal; margin-top: 8px; line-height: 1.5;">
        <sup>1</sup>Beijing Innovation Center of Humanoid Robotics&nbsp;&nbsp;
        <sup>2</sup>School of Computer Science, Peking University
      </div>

      <div class="affiliation" style="font-size: 14px; font-weight: normal; margin-top: 6px; line-height: 1.4;">
        <span><sup>*</sup>Equal contribution</span>&nbsp;&nbsp;
        <span><sup>&#8224;</sup>Project lead</span>&nbsp;&nbsp;
        <span><sup>&#8225;</sup>Co-first authors</span>&nbsp;&nbsp;
        <span><sup>&#9993;</sup>Corresponding authors</span>
      </div>
    </div>

    <div class="links-block">
      <a href="https://arxiv.org/pdf/2512.24653" target="_blank">[<strong>pdf</strong>]</a>
      <a href="https://arxiv.org/abs/2512.24653" target="_blank">[<strong>arxiv</strong>]</a>
      <a href="https://log2r.github.io/RoboMIND2.0/" target="_blank">[<strong>code</strong>]</a>
    </div>

    <div class="tagline" id="abstract">Abstract.</div>
    <div class="section">
      <p>
        While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. 
        Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited.
      </p>
      <p>
        To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310k dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. 
        Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12k tactile-enhanced episodes and 20k mobile manipulation trajectories. 
        Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20k-trajectory simulated dataset to facilitate robust sim-to-real transfer. 
      </p>
      <p>
        To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system framework optimized via offline reinforcement learning. 
        CoVLA integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions. 
        Extensive evaluations across six distinct robotic embodiments validate the effectiveness of our dataset and demonstrate that MIND-2 system significantly outperforms four single-task baselines (covering both 2D image and 3D point cloud modalities as well as four state-of-the-art VLA models).
        Furthermore, we observe that integrating tactile modalities yields measurable gains in fine-grained manipulation tasks. 
      </p>
      <p>
        Finally, experimental results show that mixing real and simulated data during training consistently enhances physical execution performance, validating both the fidelity of our simulation benchmarks and the cost-efficiency of synthetic data augmentation. 
        Our full dataset, simulation assets, and training code are publicly released to advance research in general-purpose robotic manipulation.  
      </p>
    </div>

    <img src="./figs/teaser.png"></img>
    <p class="figure-caption">
      We introduce RoboMIND 2.0, a large-scale dataset comprising 310K
dual-arm trajectories collected from six heterogeneous robot embodiments, totaling over 1,000 hours. The dataset
features rich modalities, including 12K tactile-enriched sequences and 20K mobile manipulation trajectories. Collected
through a unified teleoperation and quality assurance pipeline, RoboMIND 2.0 ensures consistent proprioception and
provides fine-grained natural language annotations. To support scalable training and evaluation, we release digital-twin
USD assets and 20K simulation trajectories aligned with real-world tasks. Building on this foundation, we propose
MIND-2, a dual-system controller that integrates a slow high-level planner MIND-2-VLM with a fast low-level policy
MIND-2-VLA, enabling robust long-horizon mobile manipulation across diverse scenarios.
    </p>

    <div class="tagline" id="collection-platform">Collection Platform.</div>

    <div class="section">
      <div style="display: flex; justify-content: flex-start; margin: 16px 0;">
        <div style="width: min(500px, 60%); text-align: center;">
          <img src="./figs/franka.png" style="width: 100%; height: auto; border-radius: 10px;" />
          <p class="figure-caption" style="text-align: left; margin-top: 10px;">
            <b>Collection platform of Franka and UR5e.</b> Collect a robotic manipulation dataset by
            controlling the dual-arm system (Franka and UR5e) via HACTS.
          </p>
        </div>
      </div>

      <div style="display: flex; justify-content: flex-end; margin: 16px 0;">
        <div style="width: min(500px, 60%); text-align: center;">
          <img src="./figs/agilex.png" style="width: 100%; height: auto; border-radius: 10px;" />
          <p class="figure-caption" style="text-align: left; margin-top: 10px;">
            <b>Collection platform of AgileX and ARX.</b> We use a VR headset to control the ARK
            robot for data collection, and employ a slave arm to teleoperate the master arm for gathering the
            Agilex manipulation dataset.
          </p>
        </div>
      </div>

      <div style="display: flex; justify-content: flex-start; margin: 16px 0;">
        <div style="width: min(500px, 60%); text-align: center;">
          <img src="./figs/tianyi.png" style="width: 100%; height: auto; border-radius: 10px;" />
          <p class="figure-caption" style="text-align: left; margin-top: 10px;">
            <b>Collection platform of Tienkung and Tianyi.</b> For the humanoid robot TienKung, data collectors will
            wear motion capture suits to record joint movements, which are then mapped to the robot to enable
            robotic manipulation. For the dual-arm mobile robot with a wheeled base Tian Yi, we collected
            datasets using two human operators.
          </p>
        </div>
      </div>
    </div>

    <div class="tagline" id="realworld-setup">Real-world Setup.</div>
    <div class="section">
      <div style="display: flex; justify-content: center; margin: 16px 0;">
        <div style="width: min(900px, 92%); text-align: center;">
          <img src="./figs/robomindexpset.png" style="width: 100%; height: auto; border-radius: 10px;" />
          <p class="figure-caption" style="text-align: left; margin-top: 10px;">
            <b>Robotic real-world setup.</b> For the Franka and UR5e robots, we use cameras positioned
            at the top, left, and right viewpoints to record the visual information of the task trajectories. For the
            humanoid (Tien Kung and Tian Yi) robots, we use their built-in RGB-D cameras to capture visual
            observations. For the AgileX and ARX robots, we use dual wrist-mounted cameras (one on each
            arm) as well as a head-mounted camera to capture visual information.
          </p>
        </div>
      </div>
    </div>

    <div class="tagline" id="realworld-results">Real-World Results.</div>
    <div class="section">
      <div style="display: flex; justify-content: center; margin: 16px 0;">
        <div style="width: min(980px, 96%); text-align: center;">
          <img src="./figs/real-res.png" style="width: 100%; height: auto; border-radius: 10px;" />
          <p class="figure-caption" style="text-align: left; margin-top: 10px;">
            Performance comparison of single task imitation learning methods and VLA models
            across different task categories.
          </p>
        </div>
      </div>
    </div>

    <!-- ===================== DUAL SYSTEM EXPERIMENTS (REPLACES TWO SECTIONS) ===================== -->
    <div class="tagline" id="dual-system-experiments">Dual System Experiments.</div>

    <div class="section">
      <!-- Figure 1 -->
      <div style="display: flex; justify-content: center; margin: 16px 0;">
        <div style="width: min(980px, 96%); text-align: center;">
          <img src="./figs/dual-mob.png" style="width: 100%; height: auto; border-radius: 10px;" />
          <p class="figure-caption" style="text-align: left; margin-top: 10px;">
            <b>Performance comparison across AgileX mobile manipulation tasks.</b> MIND-2 fast-slow system achieves significantly better
            performance across various tasks compared to both VLA models and single-task imitation learning
            methods
          </p>
        </div>
      </div>

      <!-- Figure 2 -->
      <div style="display: flex; justify-content: center; margin: 16px 0;">
        <div style="width: min(980px, 96%); text-align: center;">
          <img src="./figs/dual-col.png" style="width: 100%; height: auto; border-radius: 10px;" />
          <p class="figure-caption" style="text-align: left; margin-top: 10px;">
            <b>Success rates across three collaborative tasks.</b>
            MIND-2 (Post Training) is instantiated by fine-tuning InternVL3 and $\pi_{0.5}$ directly on data from the three multi-robot collaboration tasks.
            MIND-2 (Full-scale Training) is first pretrained on the full-scale mobile manipulation dataset using a fast-slow system architecture, and then further fine-tuned via post-training on data from three multi-robot collaboration tasks.
            MIND-2 (Offline RL), after full-scale training, we apply Implicit Q-Learning (IQL) to conduct offline reinforcement learning on the MIND-2-VLA.
          </p>
        </div>
      </div>
    </div>
    <!-- ===================== END DUAL SYSTEM EXPERIMENTS ===================== -->

    <div class="bibtex-code">
      <div class="bibtex-title">BibTeX</div>
      <pre><code>@article{cheng2025fvp,
    author    = {Chengkai Hou and Yanjie Ze and Yankai Fu and Zeyu Gao and Yue Yu and Songbo Hu and Shanghang Zhang and Huazhe Xu},
    title     = {FVP: 4D Visual Pre-training for Robot Learning},
    journal   = {ICCV},
    year      = {2025},
  }</code></pre>
    </div>
  </div> <!-- End of main-content div -->

  <div class="footer">
     Â© Shanghai Qi Zhi Institute | Webpage template from <a href="https://www.videomimic.net/">VideoMimic</a>.
  </div>

  <!-- Teaser Video Autoplay with Delay and Loop (video is commented out, so harmless) -->
  <script>
  document.addEventListener('DOMContentLoaded', function() {
    const video = document.getElementById('teaser-video');
    const loopDelay = 3000;
    let loopTimeout;

    if (video) {
      video.muted = true;

      video.addEventListener('ended', function() {
        clearTimeout(loopTimeout);
        loopTimeout = setTimeout(function() {
          video.currentTime = 0;
          video.play().catch(function(error) {
            console.log('Delayed loop play prevented for teaser video:', error);
          });
        }, loopDelay);
      });

      video.addEventListener('pause', function() {
        if (!video.ended && video.currentTime > 0) {
           clearTimeout(loopTimeout);
        }
      });
    }
  });
  </script>

  <!-- JavaScript for Video Gallery Navigation -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const galleries = [
        { sectionId: 'gallery-section-anchor', galleryInnerId: 'videoGallerySitting', scrollLeftBtnId: 'scrollLeftBtnSitting', scrollRightBtnId: 'scrollRightBtnSitting' },
        { sectionId: 'traversingGallerySection', galleryInnerId: 'videoGalleryTraversing', scrollLeftBtnId: 'scrollLeftBtnTraversing', scrollRightBtnId: 'scrollRightBtnTraversing' },
        { sectionId: 'stairsGallerySection', galleryInnerId: 'videoGalleryStairs', scrollLeftBtnId: 'scrollLeftBtnStairs', scrollRightBtnId: 'scrollRightBtnStairs' },
        { sectionId: 'reconstructionGallerySection', galleryInnerId: 'videoGalleryReconstruction', scrollLeftBtnId: 'scrollLeftBtnReconstruction', scrollRightBtnId: 'scrollRightBtnReconstruction' }
      ];

      galleries.forEach(galleryConfig => {
        const gallerySection = document.getElementById(galleryConfig.sectionId);
        if (!gallerySection) return;

        const galleryContainer = gallerySection.querySelector('.video-gallery-container');
        const galleryInner = document.getElementById(galleryConfig.galleryInnerId);
        const scrollLeftBtn = document.getElementById(galleryConfig.scrollLeftBtnId);
        const scrollRightBtn = document.getElementById(galleryConfig.scrollRightBtnId);

        if (galleryContainer && galleryInner && scrollLeftBtn && scrollRightBtn) {
          const scrollAmount = (galleryInner.firstElementChild?.offsetWidth || 300) + 15;
          scrollLeftBtn.addEventListener('click', () => galleryContainer.scrollBy({ left: -scrollAmount, behavior: 'smooth' }));
          scrollRightBtn.addEventListener('click', () => galleryContainer.scrollBy({ left: scrollAmount, behavior: 'smooth' }));
        }
      });
    });
  </script>

  <!-- JavaScript for Real-to-Sim Video Synchronization -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const videosToSync = [
        document.querySelector('.r2s-video-input'),
        document.querySelector('.r2s-video-smpl'),
        document.querySelector('.r2s-video-g1'),
        document.querySelector('.r2s-video-ego-rgb'),
        document.querySelector('.r2s-video-ego-depth'),
        document.querySelector('.r2s-video-sim')
      ].filter(Boolean);

      function synchronizeAndPlayR2SVideos() {
        if (videosToSync.length === 0) return;

        const readyPromises = videosToSync.map(video => new Promise((resolve, reject) => {
          if (video.readyState >= 4) resolve();
          else {
            video.addEventListener('canplaythrough', resolve, { once: true });
            video.addEventListener('error', reject, { once: true });
          }
        }));

        Promise.all(readyPromises).then(() => {
          videosToSync.forEach(video => {
            video.currentTime = 0;
            video.play().catch(() => { video.controls = true; });
          });
        }).catch(() => {
          videosToSync.forEach(video => video.controls = true);
        });
      }

      synchronizeAndPlayR2SVideos();
    });
  </script>

  <!-- JavaScript to prevent default click on specific image links -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const imageLinkIds = ['figure-1-img', 'figure-2-img', 'figure-3-img'];
      imageLinkIds.forEach(id => {
        const linkElement = document.getElementById(id);
        if (linkElement) linkElement.addEventListener('click', e => e.preventDefault());
      });
    });
  </script>

</body>
</html>
